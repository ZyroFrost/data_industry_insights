# -*- coding: utf-8 -*-
"""
STEP 1.1 ‚Äì RUN CRAWLERS FOR DATA JOBS SOURCES
- Crawl all data sources (ONLY missing crawler files)
- Audit CSV files generated by crawlers
"""

from pathlib import Path
import pandas as pd
import sys

# =====================================================
# FIX IMPORT PATH (KH√îNG D√ôNG python -m)
# =====================================================
ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(ROOT))

# =====================================================
# IMPORT CRAWLERS
# =====================================================
from pipeline.step1_crawlers.api.authenticated.crawl_adzuna_datajobs import run_adzuna_datajobs_crawler
from pipeline.step1_crawlers.api.authenticated.crawl_usa_datajobs import run_usa_datajobs_crawler
from pipeline.step1_crawlers.api.public.crawl_canada_datajobs import run_canada_datajobs_crawler
from pipeline.step1_crawlers.api.public.crawl_RemoteOK_datajobs import run_remoteok_datajobs_crawler

# =====================================================
# CRAWLER REGISTRY
# key = T√äN FILE CSV S·∫º XU·∫§T RA (STEP 2.0)
# =====================================================
CRAWLER_REGISTRY = {
    "adzuna_datajobs_2025.csv": {
        "origin": "CRAWLED",
        "run": run_adzuna_datajobs_crawler
    },
    "usa_government_datajobs_2020-2025.csv": {
        "origin": "CRAWLED",
        "run": run_usa_datajobs_crawler
    },
    "canada_government_datajobs_2020-2025.csv": {
        "origin": "CRAWLED",
        "run": run_canada_datajobs_crawler
    },
    "remoteok_datajobs_2025.csv": {
        "origin": "CRAWLED",
        "run": run_remoteok_datajobs_crawler
    }
}

# =====================================================
# PATH CONFIG
# =====================================================
DATA_DIR = ROOT / "data" / "data_processing" / "s2.0_data_extracted"

# =====================================================
# STEP 01 ‚Äì RUN CRAWLERS (CH·ªà KHI FILE CH∆ØA T·ªíN T·∫†I)
# =====================================================
def run_all_crawlers(data_dir: Path):
    crawler_state = {}

    for csv_name, crawler_info in CRAWLER_REGISTRY.items():
        csv_path = data_dir / csv_name
        run_fn = crawler_info["run"]

        # File ƒë√£ t·ªìn t·∫°i ‚Üí skip
        if csv_path.exists():
            crawler_state[csv_name] = "skipped"
            print(f"‚è≠ SKIP crawler (file exists): {csv_name}")
            continue

        print(f"üöÄ RUN crawler: {csv_name}")
        crawler_state[csv_name] = "running"

        try:
            run_fn()
            crawler_state[csv_name] = "done"
            print(f"‚úÖ DONE crawler: {csv_name}")
        except Exception as e:
            crawler_state[csv_name] = "fail"
            crawler_state[f"{csv_name}_error"] = str(e)
            print(f"‚ùå FAIL crawler: {csv_name}")
            print(e)

            # C√°c crawler ph√≠a sau b·ªã skip
            for k in CRAWLER_REGISTRY.keys():
                if k not in crawler_state:
                    crawler_state[k] = "skipped"
            break

    return crawler_state

# =====================================================
# CSV AUDIT UTILITIES (CH·ªà FILE CRAWLER)
# =====================================================
def count_csv_rows(path: Path) -> int:
    """Count rows in CSV (excluding header)"""
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return sum(1 for _ in f) - 1


def audit_csv_basic(data_dir: Path):
    print("\nüìä Auditing crawler CSV files...\n")

    records = []
    CRAWLED_FILES = set(CRAWLER_REGISTRY.keys())

    for csv_file in data_dir.glob("*.csv"):
        if csv_file.name not in CRAWLED_FILES:
            continue  # b·ªè qua external files

        try:
            df_head = pd.read_csv(csv_file, nrows=0)
            records.append({
                "file_name": csv_file.name,
                "row_count": count_csv_rows(csv_file),
                "column_count": len(df_head.columns)
            })
        except Exception:
            records.append({
                "file_name": csv_file.name,
                "row_count": None,
                "column_count": None
            })

    df = pd.DataFrame(records)

    df["row_count"] = pd.to_numeric(df["row_count"], errors="coerce")
    df["column_count"] = pd.to_numeric(df["column_count"], errors="coerce")

    summary = {
        "total_files": int(len(df)),
        "total_rows": int(df["row_count"].sum()),
        "total_columns": int(df["column_count"].sum()),
        "error_files": int(df["row_count"].isna().sum())
    }

    print("üóÇ CSV DETAIL")
    print(df.sort_values("file_name"))

    print("\nüìå SUMMARY")
    for k, v in summary.items():
        print(f"- {k}: {v}")

    return df, summary

# =====================================================
# MAIN
# =====================================================
if __name__ == "__main__":
    print("=" * 60)
    print("DATA INDUSTRY INSIGHTS ‚Äì STEP 01")
    print("=" * 60)

    crawler_state = run_all_crawlers(DATA_DIR)
    audit_csv_basic(DATA_DIR)

    print("\nüéâ STEP 01 DONE")
